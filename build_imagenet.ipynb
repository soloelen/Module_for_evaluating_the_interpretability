{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import requests\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "import PIL\n",
    "from PIL.Image import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from src.interpretation.interpretation_functions import layer_cam_gen, guided_backprop_gen, gradpp_cam_gen, saliency_gen, grad_cam_gen\n",
    "from src.interpretation.image_utils import im_show, preprocess, ClassifierOutputSoftmaxTarget, \\\n",
    "    confidence_change_apply_cam, \\\n",
    "    get_target_index, preprocess_image, view, to_plot_bbox\n",
    "from src.interpretation.metrics import deletion_metric, deletion_game, preservation_metric, preservation_game, \\\n",
    "    average_drop_item, \\\n",
    "    avg_drop_list, increase_in_confidence_item, increase_in_confidence_list, topk_img, sparsity, iou_loc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель EfficientNetB0      \n",
    "Deletion Game diffs=0.0012688534 perc=10.0  \n",
    "Preservation Game diffs=0.00013 perc= 990.0\n",
    "Avg Drop 81.57\n",
    "Increase in Confidence  0.02  \n",
    "Sparsity 5.0424   \n",
    "\n",
    "Модель VGG      \n",
    "Deletion Game diffs=0.12864499, perc=10.0  \n",
    "Preservation Game diffs=0.06659404 perc= 990.0\n",
    "Avg Drop 89.966\n",
    "Increase in Confidence 0.05\n",
    "Sparsity 3.39178  \n",
    "Модель Resnet      \n",
    "Deletion Game diffs=0.02668962 perc=1.0\n",
    "Preservation Game diffs=0.01636001 perc= 990.0\n",
    "Avg Drop 87.85\n",
    "Increase in Confidence  0.03\n",
    "Sparsity 2.5085 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0(weights=\"IMAGENET1K_V1\").to(device)\n",
    "model.eval()\n",
    "transforms = models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib import patches\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "with open(\"export-result (imagenett_FINAL).ndjson\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "list_iou = []\n",
    "\n",
    "for i in range(len(data[\"a\"])):\n",
    "  name_dict = data[\"a\"][i][\"data_row\"][\"external_id\"]\n",
    "  \n",
    "  bbox_dict_i = data[\"a\"][i][\"projects\"][list(data[\"a\"][i][\"projects\"].keys())[0]][\"labels\"][0][\"annotations\"][\"objects\"][0][\"bounding_box\"]\n",
    "  bb_top_i = bbox_dict_i[\"top\"]\n",
    "  bb_left_i = bbox_dict_i[\"left\"]\n",
    "  bb_height_i = bbox_dict_i[\"height\"]\n",
    "  bb_width_i = bbox_dict_i[\"width\"]\n",
    "  im = Image.open(f\"data_labeling_imagenet/{name_dict}\")\n",
    "  res, gray_res = guided_backprop_gen(model, \n",
    "                               im_show(f\"data_labeling_imagenet/{name_dict}\"),\n",
    "                               target_layers= [model.features[-1]])\n",
    "  bb_arr = np.array([bb_top_i, bb_left_i, bb_top_i + bb_height_i, bb_left_i + bb_width_i])\n",
    "  iou_i = iou_loc(bb_arr, gray_res)\n",
    "  list_iou.append(iou_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IOU Average: {np.mean(list_iou)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    raw_images = []\n",
    "    print(\"Images:\")\n",
    "    for i, image_path in enumerate([image_paths]):\n",
    "        print(\"\\t#{}: {}\".format(i, image_path))\n",
    "        \n",
    "        image, raw_image = preprocess(image_path)\n",
    "        images.append(image)\n",
    "        raw_images.append(raw_image)\n",
    "    return images, raw_images\n",
    "\n",
    "def get_device(cuda):\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    if cuda:\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
    "    else:\n",
    "        print(\"Device: CPU\")\n",
    "    return device\n",
    "\n",
    "def save_sensitivity(filename, maps):\n",
    "    maps = maps.cpu().numpy()\n",
    "    scale = max(maps[maps > 0].max(), -maps[maps <= 0].min())\n",
    "    maps = maps / scale * 0.5\n",
    "    maps += 0.5\n",
    "    maps = cm.bwr_r(maps)[..., :3]\n",
    "    maps = np.uint8(maps * 255.0)\n",
    "    maps = cv2.resize(maps, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(filename, maps)\n",
    "    \n",
    "def preprocess(img):\n",
    "    raw_image = cv2.resize(img, (224, 224))\n",
    "    image = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )(raw_image[..., ::-1].copy())\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from collections import Sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def occlusion_sensitivity(model, \n",
    "                          images, \n",
    "                          ids, \n",
    "                          mean=None, \n",
    "                          patch=35, \n",
    "                          stride=1, \n",
    "                          n_batches=128):\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.eval()\n",
    "    mean = mean if mean else 0\n",
    "    patch_H, patch_W = patch if isinstance(patch, Sequence) else (patch, patch)\n",
    "    pad_H, pad_W = patch_H // 2, patch_W // 2\n",
    "\n",
    "    # Padded image\n",
    "    images = F.pad(images, (pad_W, pad_W, pad_H, pad_H), value=mean)\n",
    "    B, _, H, W = images.shape\n",
    "    new_H = (H - patch_H) // stride + 1\n",
    "    new_W = (W - patch_W) // stride + 1\n",
    "\n",
    "    # Prepare sampling grids\n",
    "    anchors = []\n",
    "    grid_h = 0\n",
    "    while grid_h <= H - patch_H:\n",
    "        grid_w = 0\n",
    "        while grid_w <= W - patch_W:\n",
    "            grid_w += stride\n",
    "            anchors.append((grid_h, grid_w))\n",
    "        grid_h += stride\n",
    "\n",
    "    # Baseline score without occlusion\n",
    "    baseline = model(images).detach().gather(1, ids)\n",
    "\n",
    "    # Compute per-pixel logits\n",
    "    scoremaps = []\n",
    "    for i in tqdm(range(0, len(anchors), n_batches), leave=False):\n",
    "        batch_images = []\n",
    "        batch_ids = []\n",
    "        for grid_h, grid_w in anchors[i : i + n_batches]:\n",
    "            images_ = images.clone()\n",
    "            images_[..., grid_h : grid_h + patch_H, grid_w : grid_w + patch_W] = mean\n",
    "            batch_images.append(images_)\n",
    "            batch_ids.append(ids)\n",
    "        batch_images = torch.cat(batch_images, dim=0)\n",
    "        batch_ids = torch.cat(batch_ids, dim=0)\n",
    "        scores = model(batch_images).detach().gather(1, batch_ids)\n",
    "        scoremaps += list(torch.split(scores, B))\n",
    "\n",
    "    diffmaps = torch.cat(scoremaps, dim=1) - baseline\n",
    "    diffmaps = diffmaps.view(B, new_H, new_W)\n",
    "\n",
    "    return diffmaps\n",
    "\n",
    "def occlusion_sens_gen(images, model, topk, stride, n_batches):\n",
    "    \n",
    "    print(\"Occlusion Sensitivity:\")\n",
    "\n",
    "    patche_sizes = [10,]# 15, 50]\n",
    "\n",
    "    images = [torchvision.transforms.ToTensor()(i) for i in images]\n",
    "    images = torch.stack(images).to(device)\n",
    "    logits = model(images)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs, ids = probs.sort(dim=1, descending=True)\n",
    "    res = []\n",
    "    for i in range(topk):\n",
    "        for p in patche_sizes:\n",
    "            print(\"Patch:\", p)\n",
    "            sensitivity = occlusion_sensitivity(\n",
    "                model, images, ids[:, [i]], patch=p, stride=stride, n_batches=n_batches\n",
    "            )\n",
    "        res.append(sensitivity)\n",
    "    return res\n",
    "\n",
    "def topk_img(input_image, cam, k):\n",
    "  topk = np.argsort(cam.flatten())[-k:]\n",
    "  topk = np.stack([topk % 224, topk // 224], axis=1)\n",
    "  out_image = input_image * 255\n",
    "  out_image[topk[:, 1], topk[:, 0]] = 0\n",
    "  # assert (out_image != input_image).sum() != 0\n",
    "  return PIL.Image.fromarray(out_image.reshape(224, 224, 3).astype(np.uint8), 'RGB')\n",
    "\n",
    "res = occlusion_sens_gen(np.stack(imgs)[:1], model, topk=1, stride=1, n_batches=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res[0].cpu().numpy() * 255).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = r'https://www.purina.co.uk/sites/default/files/styles/ttt_image_510/public/2020-11/Should%20I%20Get%20a%20Cat%20or%20Dog1.jpg?itok=IdntHkbV'\n",
    "img_path = './data/dog.jpeg'\n",
    "urllib.request.urlretrieve(img_url, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, gray_res = layer_cam_gen(model,\n",
    "                              im_show(img_path),\n",
    "                              target_layers=[model.features[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/imagenette_sample/\"\n",
    "imagenet_files = os.listdir(path)\n",
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    res, gray_res = layer_cam_gen(model, im_show(path + file), target_layers=[model.features[-1]])\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res)\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    #plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    res, gray_res = gradpp_cam_gen(model, im_show(path + file), target_layers=[model.features[-1]])\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res)\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    #plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    tensor = transform(im_show(path + file)).to(device)\n",
    "    res = saliency_gen(tensor, model)\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res.cpu().numpy())\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    #plt.imshow(res)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "files = np.random.choice(glob(\"data/imagenette_train/*/*\"), 100)\n",
    "t = torchvision.transforms.ToTensor()\n",
    "imgs = [im_show(f) for f in files]\n",
    "'''imgs = torch.stack(imgs)\n",
    "imgs = torchvision.transforms.Resize((224, 224))(imgs)\n",
    "gray = []\n",
    "for f in files:\n",
    "    res, gray_res = layer_cam_gen(model,\n",
    "                              im_show(f),\n",
    "                              target_layers=[model.features[-1]])\n",
    "    gray.append(gray_res)    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 217,\n",
    "    \"n02979186\": 482,\n",
    "    \"n03000684\": 491,\n",
    "    \"n03028079\": 497,\n",
    "    \"n03394916\": 566,\n",
    "    \"n03417042\": 569,\n",
    "    \"n03425413\": 571,\n",
    "    \"n03445777\": 574,\n",
    "    \"n03888257\": 701\n",
    "}\n",
    "\n",
    "labels = [label_dict[f.split(\"\\\\\")[1]]for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gray = []\n",
    "for img in imgs:\n",
    "    res, gray_res = guided_backprop_gen(model, img, target_layers=[model.features[-1]])\n",
    "    gray.append(gray_res)   \n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "diffs, percs = [], []\n",
    "for img, g in zip(imgs, gray):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)\n",
    "\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for img in imgs:\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===GUIDED BACKPROP===\")\n",
    "print(\"Deletion game:\", np.mean(diffs), np.mean(perc))\n",
    "print(\"Preservation game:\", np.mean(diffs_preservation), np.mean(percs_preservation))\n",
    "print(\"Avg drop:\", avg_drop_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, index=labels))\n",
    "print(\"Increace in Confidence:\", increase_in_confidence_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, labels))\n",
    "print(\"Sparsity\", sparsity(cam=gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gray = []\n",
    "for img in imgs:\n",
    "    res, gray_res = grad_cam_gen(model, img, target_layers=[model.features[-1]])\n",
    "    gray.append(gray_res)   \n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "diffs, percs = [], []\n",
    "for img, g in zip(imgs, gray):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)\n",
    "\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for img in imgs:\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===GRAD CAM===\")\n",
    "print(\"Deletion game:\", np.mean(diffs), np.mean(perc))\n",
    "print(\"Preservation game:\", np.mean(diffs_preservation), np.mean(percs_preservation))\n",
    "print(\"Avg drop:\", avg_drop_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, index=labels))\n",
    "print(\"Increace in Confidence:\", increase_in_confidence_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, labels))\n",
    "print(\"Sparsity\", sparsity(cam=gray))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deletion Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "diffs, percs = [], []\n",
    "for f, g in zip(files, gray):\n",
    "    img = torchvision.io.read_image(f, mode=torchvision.io.ImageReadMode.RGB).float().unsqueeze(0)\n",
    "    img = transform(img)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(diffs), np.mean(perc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preservation Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for f, g in zip(files, gray):\n",
    "    img = torchvision.io.read_image(f, mode=torchvision.io.ImageReadMode.RGB).float().unsqueeze(0)\n",
    "    img = transform(img)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(diffs_preservation), np.mean(percs_preservation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 217,\n",
    "    \"n02979186\": 482,\n",
    "    \"n03000684\": 491,\n",
    "    \"n03028079\": 497,\n",
    "    \"n03394916\": 566,\n",
    "    \"n03417042\": 569,\n",
    "    \"n03425413\": 571,\n",
    "    \"n03445777\": 574,\n",
    "    \"n03888257\": 701\n",
    "}\n",
    "\n",
    "labels = [label_dict[f.split(\"\\\\\")[1]]for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_drop_list(imgs.unsqueeze(1), np.array(gray), model, index=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase in Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_in_confidence_list(imgs.unsqueeze(1), np.array(gray), model, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Sparsity = {sparsity(cam=gray)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights='DEFAULT').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_iou = []\n",
    "\n",
    "for i in range(len(data[\"a\"])):\n",
    "  name_dict = data[\"a\"][i][\"data_row\"][\"external_id\"]\n",
    "  \n",
    "  bbox_dict_i = data[\"a\"][i][\"projects\"][list(data[\"a\"][i][\"projects\"].keys())[0]][\"labels\"][0][\"annotations\"][\"objects\"][0][\"bounding_box\"]\n",
    "  bb_top_i = bbox_dict_i[\"top\"]\n",
    "  bb_left_i = bbox_dict_i[\"left\"]\n",
    "  bb_height_i = bbox_dict_i[\"height\"]\n",
    "  bb_width_i = bbox_dict_i[\"width\"]\n",
    "  im = Image.open(f\"data_labeling_imagenet/{name_dict}\")\n",
    "  res, gray_res = guided_backprop_gen(model, \n",
    "                               im_show(f\"data_labeling_imagenet/{name_dict}\"),\n",
    "                               target_layers= [model.features[-1]])\n",
    "  bb_arr = np.array([bb_top_i, bb_left_i, bb_top_i + bb_height_i, bb_left_i + bb_width_i])\n",
    "  iou_i = iou_loc(bb_arr, gray_res)\n",
    "  list_iou.append(iou_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IOU Average: {np.mean(list_iou)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gray = []\n",
    "for img in imgs:\n",
    "    res, gray_res = guided_backprop_gen(model, img, target_layers=[model.features[-1]])\n",
    "    gray.append(gray_res)   \n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "diffs, percs = [], []\n",
    "for img, g in zip(imgs, gray):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)\n",
    "\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for img in imgs:\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===GUIDED BACKPROP===\")\n",
    "print(\"Deletion game:\", np.mean(diffs), np.mean(perc))\n",
    "print(\"Preservation game:\", np.mean(diffs_preservation), np.mean(percs_preservation))\n",
    "print(\"Avg drop:\", avg_drop_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, index=labels))\n",
    "print(\"Increace in Confidence:\", increase_in_confidence_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, labels))\n",
    "print(\"Sparsity\", sparsity(cam=gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gray = []\n",
    "for img in imgs:\n",
    "    res, gray_res = grad_cam_gen(model, img, target_layers=[model.features[-1]])\n",
    "    gray.append(gray_res)   \n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "diffs, percs = [], []\n",
    "for img, g in zip(imgs, gray):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)\n",
    "\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for img in imgs:\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===GRAD CAM===\")\n",
    "print(\"Deletion game:\", np.mean(diffs), np.mean(perc))\n",
    "print(\"Preservation game:\", np.mean(diffs_preservation), np.mean(percs_preservation))\n",
    "print(\"Avg drop:\", avg_drop_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, index=labels))\n",
    "print(\"Increace in Confidence:\", increase_in_confidence_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, labels))\n",
    "print(\"Sparsity\", sparsity(cam=gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/imagenette_sample/\"\n",
    "imagenet_files = os.listdir(path)\n",
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    res, gray_res = layer_cam_gen(model, im_show(path + file), target_layers=[model.avgpool])\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res)\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    #plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    res, gray_res = gradpp_cam_gen(model, im_show(path + file), target_layers=[model.features[-1]])\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res)\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    #plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    tensor = transform(im_show(path + file)).to(device)\n",
    "    res = saliency_gen(tensor, model)\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res.cpu().numpy())\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    #plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "files = np.random.choice(glob(\"data/imagenette_train/*/*\"), 100)\n",
    "t = torchvision.transforms.ToTensor()\n",
    "imgs = [t(im_show(f)) for f in files]\n",
    "imgs = torch.stack(imgs)\n",
    "imgs = torchvision.transforms.Resize((224, 224))(imgs)\n",
    "gray = []\n",
    "for f in files:\n",
    "    res, gray_res = layer_cam_gen(model,\n",
    "                              im_show(f),\n",
    "                              target_layers=[model.features[-1]])\n",
    "    gray.append(gray_res)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deletion Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "diffs, percs = [], []\n",
    "for f, g in zip(files, gray):\n",
    "    img = torchvision.io.read_image(f, mode=torchvision.io.ImageReadMode.RGB).float().unsqueeze(0)\n",
    "    img = transform(img)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(diffs), np.mean(perc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preservation Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for f, g in zip(files, gray):\n",
    "    img = torchvision.io.read_image(f, mode=torchvision.io.ImageReadMode.RGB).float().unsqueeze(0)\n",
    "    img = transform(img)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(diffs_preservation), np.mean(percs_preservation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 217,\n",
    "    \"n02979186\": 482,\n",
    "    \"n03000684\": 491,\n",
    "    \"n03028079\": 497,\n",
    "    \"n03394916\": 566,\n",
    "    \"n03417042\": 569,\n",
    "    \"n03425413\": 571,\n",
    "    \"n03445777\": 574,\n",
    "    \"n03888257\": 701\n",
    "}\n",
    "\n",
    "labels = [label_dict[f.split(\"\\\\\")[1]]for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_drop_list(imgs.unsqueeze(1), np.array(gray), model, index=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increace in Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_in_confidence_list(imgs.unsqueeze(1), np.array(gray), model, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Sparsity = {sparsity(cam=gray)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_iou = []\n",
    "\n",
    "for i in range(len(data[\"a\"])):\n",
    "  name_dict = data[\"a\"][i][\"data_row\"][\"external_id\"]\n",
    "  \n",
    "  bbox_dict_i = data[\"a\"][i][\"projects\"][list(data[\"a\"][i][\"projects\"].keys())[0]][\"labels\"][0][\"annotations\"][\"objects\"][0][\"bounding_box\"]\n",
    "  bb_top_i = bbox_dict_i[\"top\"]\n",
    "  bb_left_i = bbox_dict_i[\"left\"]\n",
    "  bb_height_i = bbox_dict_i[\"height\"]\n",
    "  bb_width_i = bbox_dict_i[\"width\"]\n",
    "  im = Image.open(f\"data_labeling_imagenet/{name_dict}\")\n",
    "  res, gray_res = guided_backprop_gen(model, \n",
    "                               im_show(f\"data_labeling_imagenet/{name_dict}\"),\n",
    "                               target_layers= [model.layer4])\n",
    "  bb_arr = np.array([bb_top_i, bb_left_i, bb_top_i + bb_height_i, bb_left_i + bb_width_i])\n",
    "  iou_i = iou_loc(bb_arr, gray_res)\n",
    "  list_iou.append(iou_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IOU Average: {np.mean(list_iou)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gray = []\n",
    "for img in imgs:\n",
    "    res, gray_res = guided_backprop_gen(model, img, target_layers=[model.layer4])\n",
    "    gray.append(gray_res)   \n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "diffs, percs = [], []\n",
    "for img, g in zip(imgs, gray):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)\n",
    "\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for img in imgs:\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===GUIDED BACKPROP===\")\n",
    "print(\"Deletion game:\", np.mean(diffs), np.mean(perc))\n",
    "print(\"Preservation game:\", np.mean(diffs_preservation), np.mean(percs_preservation))\n",
    "print(\"Avg drop:\", avg_drop_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, index=labels))\n",
    "print(\"Increace in Confidence:\", increase_in_confidence_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, labels))\n",
    "print(\"Sparsity\", sparsity(cam=gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gray = []\n",
    "for img in imgs:\n",
    "    res, gray_res = grad_cam_gen(model, img, target_layers=[model.layer4])\n",
    "    gray.append(gray_res)   \n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "diffs, percs = [], []\n",
    "for img, g in zip(imgs, gray):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)\n",
    "\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for img in imgs:\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===GRAD CAM===\")\n",
    "print(\"Deletion game:\", np.mean(diffs), np.mean(perc))\n",
    "print(\"Preservation game:\", np.mean(diffs_preservation), np.mean(percs_preservation))\n",
    "print(\"Avg drop:\", avg_drop_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, index=labels))\n",
    "print(\"Increace in Confidence:\", increase_in_confidence_list([transform(b).unsqueeze(0) for b in imgs], np.array(gray), model, labels))\n",
    "print(\"Sparsity\", sparsity(cam=gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/imagenette_sample/\"\n",
    "imagenet_files = os.listdir(path)\n",
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    res, gray_res = layer_cam_gen(model, im_show(\n",
    "        path + file), target_layers=[model.layer4])\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res)\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    # plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    res, gray_res = gradpp_cam_gen(model, im_show(\n",
    "        path + file), target_layers=[model.layer4])\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res)\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    # plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in imagenet_files:\n",
    "    #img = transforms(torchvision.io.read_image(path + file)).float()\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    tensor = transform(im_show(path + file)).to(device)\n",
    "    res = saliency_gen(tensor, model)\n",
    "    f, axarr = plt.subplots(1, 1)\n",
    "    axarr.imshow(res.cpu().numpy())\n",
    "    #plt.imshow(torch.movedim(torchvision.io.read_image(path + file), 0, -1).numpy())\n",
    "    # plt.imshow(res)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "files = np.random.choice(glob(\"data/imagenette_train/*/*\"), 100)\n",
    "t = torchvision.transforms.ToTensor()\n",
    "imgs = [t(im_show(f)) for f in files]\n",
    "imgs = torch.stack(imgs)\n",
    "imgs = torchvision.transforms.Resize((224, 224))(imgs)\n",
    "gray = []\n",
    "for f in files:\n",
    "    res, gray_res = layer_cam_gen(model,\n",
    "                              im_show(f),\n",
    "                              target_layers=[model.layer4])\n",
    "    gray.append(gray_res)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deletion Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "diffs, percs = [], []\n",
    "for f, g in zip(files, gray):\n",
    "    img = torchvision.io.read_image(f, mode=torchvision.io.ImageReadMode.RGB).float().unsqueeze(0)\n",
    "    img = transform(img)\n",
    "    diff_, perc, cam_ = deletion_game(img.to(device), cam=g, model=model, gray_res=g)\n",
    "    diffs.append(diff_), percs.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(diffs), np.mean(perc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preservation Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Resize((224, 224))\n",
    "diffs_preservation, percs_preservation = [], []\n",
    "for f, g in zip(files, gray):\n",
    "    img = torchvision.io.read_image(f, mode=torchvision.io.ImageReadMode.RGB).float().unsqueeze(0)\n",
    "    img = transform(img)\n",
    "    diff_, perc, cam_ = preservation_game(img.to(device), cam=g, model=model, cam_=g)\n",
    "    diffs_preservation.append(diff_), percs_preservation.append(perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(diffs_preservation), np.mean(percs_preservation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"n01440764\": 0,\n",
    "    \"n02102040\": 217,\n",
    "    \"n02979186\": 482,\n",
    "    \"n03000684\": 491,\n",
    "    \"n03028079\": 497,\n",
    "    \"n03394916\": 566,\n",
    "    \"n03417042\": 569,\n",
    "    \"n03425413\": 571,\n",
    "    \"n03445777\": 574,\n",
    "    \"n03888257\": 701\n",
    "}\n",
    "\n",
    "labels = [label_dict[f.split(\"\\\\\")[1]]for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_drop_list(imgs.unsqueeze(1), np.array(gray), model, index=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase in Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_in_confidence_list(imgs.unsqueeze(1), np.array(gray), model, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Sparsity = {sparsity(cam=gray)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
