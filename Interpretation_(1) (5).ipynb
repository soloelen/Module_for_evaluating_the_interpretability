{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9Mq4tI2dfL5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Mq4tI2dfL5",
        "outputId": "c3ad3e39-1c0e-4d2e-bd28-ac863a850dbd"
      },
      "outputs": [],
      "source": [
        "!pip install grad-cam"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a3c45615-eb49-4ea1-8d8f-9954e425a7b0",
      "metadata": {
        "id": "a3c45615-eb49-4ea1-8d8f-9954e425a7b0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d713f6fa-a0f1-440c-956b-f87277868e57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d713f6fa-a0f1-440c-956b-f87277868e57",
        "outputId": "11562430-a23b-4010-e7e8-64e96eaec09a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "cudnn.benchmark = True  # fire on all cylinders\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from copy import copy\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "from PIL import Image, ImageFilter\n",
        "import urllib\n",
        "import matplotlib.cm as cm\n",
        "from collections import Sequence\n",
        "import matplotlib\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor\n",
        "from typing import List, Dict\n",
        "import math\n",
        "\n",
        "\n",
        "import cv2\n",
        "from pytorch_grad_cam import EigenCAM, EigenGradCAM, LayerCAM\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, GradCAMElementWise\n",
        "from pytorch_grad_cam import AblationCAM, RandomCAM, FullGrad, ScoreCAM, HiResCAM, XGradCAM\n",
        "from pytorch_grad_cam.guided_backprop import GuidedBackpropReLUModel\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, scale_cam_image\n",
        "\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c83cda7-1619-4b17-8263-d14758bcf55f",
      "metadata": {
        "id": "1c83cda7-1619-4b17-8263-d14758bcf55f"
      },
      "outputs": [],
      "source": [
        "# img_url_ = r'https://cdn.psychologytoday.com/sites/default/files/styles/article-inline-half-caption/public/field_blog_entry_images/2020-04/cb.jpg?itok=zzuVtGPr'\n",
        "img_url = r'https://www.purina.co.uk/sites/default/files/styles/ttt_image_510/public/2020-11/Should%20I%20Get%20a%20Cat%20or%20Dog1.jpg?itok=IdntHkbV'\n",
        "img_name = 'dog.jpeg'\n",
        "urllib.request.urlretrieve(img_url, img_name)\n",
        "# image = \"/home/results/korgi.jpeg\"\n",
        "image = img_name\n",
        "device = 'cuda'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "01caab38-daeb-4045-bfa4-f78624fe321f",
      "metadata": {
        "id": "01caab38-daeb-4045-bfa4-f78624fe321f"
      },
      "source": [
        "### Util functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445ef844-e138-487d-bc1e-64b866b8f575",
      "metadata": {
        "id": "445ef844-e138-487d-bc1e-64b866b8f575"
      },
      "outputs": [],
      "source": [
        "def im_show(im_path):\n",
        "    \n",
        "    img = np.array(Image.open(im_path))\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    rgb_img = img.copy()\n",
        "    img = np.float32(img) / 255\n",
        "    plt.imshow(img)\n",
        "    \n",
        "    return img\n",
        "\n",
        "def load_images(image_paths):\n",
        "    images = []\n",
        "    raw_images = []\n",
        "    print(\"Images:\")\n",
        "    for i, image_path in enumerate([image_paths]):\n",
        "        print(\"\\t#{}: {}\".format(i, image_path))\n",
        "        \n",
        "        image, raw_image = preprocess(image_path)\n",
        "        images.append(image)\n",
        "        raw_images.append(raw_image)\n",
        "    return images, raw_images\n",
        "\n",
        "def get_device(cuda):\n",
        "    cuda = cuda and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    if cuda:\n",
        "        current_device = torch.cuda.current_device()\n",
        "        print(\"Device:\", torch.cuda.get_device_name(current_device))\n",
        "    else:\n",
        "        print(\"Device: CPU\")\n",
        "    return device\n",
        "\n",
        "def save_sensitivity(filename, maps):\n",
        "    maps = maps.cpu().numpy()\n",
        "    scale = max(maps[maps > 0].max(), -maps[maps <= 0].min())\n",
        "    maps = maps / scale * 0.5\n",
        "    maps += 0.5\n",
        "    maps = cm.bwr_r(maps)[..., :3]\n",
        "    maps = np.uint8(maps * 255.0)\n",
        "    maps = cv2.resize(maps, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "    cv2.imwrite(filename, maps)\n",
        "    \n",
        "def preprocess(image_path):\n",
        "    raw_image = cv2.imread(image_path)\n",
        "    # print(raw_image.shape)\n",
        "    raw_image = cv2.resize(raw_image, (224, 224))\n",
        "    image = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ]\n",
        "    )(raw_image[..., ::-1].copy())\n",
        "    return image, raw_image\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b7100276-2de4-4ce5-b19e-75165280eda0",
      "metadata": {
        "id": "b7100276-2de4-4ce5-b19e-75165280eda0",
        "tags": []
      },
      "source": [
        "### Interpretation-functions\n",
        "**Common methods**\n",
        "- Saliency maps - done\n",
        "- Occlussion sensativity - done\n",
        "- Integrated Gradients - **should be tested**\n",
        "- LRPs\n",
        "- Deep Taylor Decomposition\n",
        "\n",
        "**Backprops**\n",
        "- VanilaBackprop\n",
        "- GuidedBackprop - done\n",
        "\n",
        "**CAMs**\n",
        "- GradCAM - done\n",
        "- HiResCAM - done\n",
        "- GradCAM ElementWise - done\n",
        "- GradCAM++ - done\n",
        "- XGradCAM - done\n",
        "- AblationCAM - done\n",
        "- ScoreCAM - done\n",
        "- EigenCAM - done\n",
        "- EigenGradCAM - done\n",
        "- LayerCAM- done \n",
        "- RandomCAM - done\n",
        "- FullGrad - done\n",
        "- Deep Feature Factorizations\n",
        "\n",
        "**Global methods**\n",
        "- SHAP\n",
        "- LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5db7151-b195-432c-a15d-a3c60f9a98ed",
      "metadata": {
        "id": "c5db7151-b195-432c-a15d-a3c60f9a98ed"
      },
      "outputs": [],
      "source": [
        "def eigen_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = EigenCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def eigengrad_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = EigenGradCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def grad_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = GradCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def gradpp_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = GradCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "# --------\n",
        "\n",
        "def ablation_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = AblationCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def random_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = RandomCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def fullgrad_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = FullGrad(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def score_cam_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = ScoreCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def hires_cam_gen(model, img, target_layers): # need test\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = HiResCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def elw_grad_cam_gen(model, img, target_layers): # need test\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = GradCAMElementWise(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def xgrad_cam_gen(model, img, target_layers): # need test\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = XGradCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def layer_cam_gen(model, img, target_layers): # need test\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.requires_grad_()\n",
        "    tensor.to(device)\n",
        "    cam = LayerCAM(model, target_layers, use_cuda= True)\n",
        "    grayscale_cam = cam(tensor)[0, :, :]\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "#-----------\n",
        "\n",
        "def guided_backprop_gen(model, img, target_layers):\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(img).unsqueeze(0)\n",
        "    tensor.to(device)\n",
        "    cam = GuidedBackpropReLUModel(model, target_layers)\n",
        "    grayscale_cam = cam(tensor)[:, :, 0]\n",
        "    print(grayscale_cam.shape)\n",
        "    cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
        "    return cam_image, grayscale_cam\n",
        "\n",
        "def saliency_gen(img, model):\n",
        "    # we don't need gradients weights for a trained model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.eval()\n",
        "    input = img\n",
        "    input.unsqueeze_(0)\n",
        "    input.requires_grad = True\n",
        "    preds = model(input)\n",
        "    score, indices = torch.max(preds, 1)\n",
        "    score.backward()\n",
        "    slc, _ = torch.max(torch.abs(input.grad[0]), dim=0)\n",
        "\n",
        "    return slc\n",
        "\n",
        "\n",
        "# Integrated gradients should be updated. TODO: issue with \"grad\" \n",
        "def integrated_grads_gen(batch_x, model, batch_blank_type= 'zero', iterations = 100):\n",
        "    mean_grad = 0\n",
        "    \n",
        "    transform = transforms.ToTensor()\n",
        "    batch_x = transform(batch_x).unsqueeze(0)\n",
        "    batch_x = batch_x.to(device)\n",
        "    model.to(device)\n",
        "    \n",
        "    if batch_blank_type == 'zero':\n",
        "        batch_blank = torch.zeros_like(batch_x)\n",
        "    elif batch_blank_type == 'one':\n",
        "        batch_blank = torch.ones_like(batch_x)\n",
        "    elif batch_blank_type == 'rand':\n",
        "        batch_blank = torch.rand_like(batch_x)\n",
        "    \n",
        "    batch_blank = batch_blank.to(device)\n",
        "    \n",
        "    for i in tqdm(range(1, iterations + 1)):\n",
        "        k = i / iterations\n",
        "        x = batch_blank + k * (batch_x - batch_blank)\n",
        "        x = Variable(x, requires_grad = True)\n",
        "        x = x.to(device)\n",
        "        \n",
        "        with torch.enable_grad():\n",
        "            outputs = model(x)\n",
        "            \n",
        "            value, preds = torch.max(outputs, 1)\n",
        "\n",
        "            print(value, preds)\n",
        "            predictions = preds.type(torch.cuda.FloatTensor)\n",
        "            predictions = Variable(predictions, requires_grad = True)\n",
        "            predictions.retain_grad()\n",
        "            \n",
        "            # Comment underline is a 1st approach to get grads\n",
        "            # predictions.backward(retain_graph=True)\n",
        "            # print(x.grad)\n",
        "            # grad = x.grad\n",
        "            \n",
        "            # Comment underline is a 2nd approach to get grads\n",
        "            (grad,) = torch.autograd.grad(predictions, x,  allow_unused = True)\n",
        "            \n",
        "            # grad = grad.retain_grad()\n",
        "        \n",
        "         \n",
        "        print('iter = ', i, 'grad ', grad)\n",
        "        if grad == None:\n",
        "            grad = 0\n",
        "        mean_grad += grad / iterations\n",
        "\n",
        "    integrated_gradients = (batch_x - batch_blank) * mean_grad\n",
        "\n",
        "    return integrated_gradients, mean_grad\n",
        "\n",
        "def occlusion_sensitivity(model, \n",
        "                          images, \n",
        "                          ids, \n",
        "                          mean=None, \n",
        "                          patch=35, \n",
        "                          stride=1, \n",
        "                          n_batches=128):\n",
        "\n",
        "    torch.set_grad_enabled(False)\n",
        "    model.eval()\n",
        "    mean = mean if mean else 0\n",
        "    patch_H, patch_W = patch if isinstance(patch, Sequence) else (patch, patch)\n",
        "    pad_H, pad_W = patch_H // 2, patch_W // 2\n",
        "\n",
        "    # Padded image\n",
        "    images = F.pad(images, (pad_W, pad_W, pad_H, pad_H), value=mean)\n",
        "    B, _, H, W = images.shape\n",
        "    new_H = (H - patch_H) // stride + 1\n",
        "    new_W = (W - patch_W) // stride + 1\n",
        "\n",
        "    # Prepare sampling grids\n",
        "    anchors = []\n",
        "    grid_h = 0\n",
        "    while grid_h <= H - patch_H:\n",
        "        grid_w = 0\n",
        "        while grid_w <= W - patch_W:\n",
        "            grid_w += stride\n",
        "            anchors.append((grid_h, grid_w))\n",
        "        grid_h += stride\n",
        "\n",
        "    # Baseline score without occlusion\n",
        "    baseline = model(images).detach().gather(1, ids)\n",
        "\n",
        "    # Compute per-pixel logits\n",
        "    scoremaps = []\n",
        "    for i in tqdm(range(0, len(anchors), n_batches), leave=False):\n",
        "        batch_images = []\n",
        "        batch_ids = []\n",
        "        for grid_h, grid_w in anchors[i : i + n_batches]:\n",
        "            images_ = images.clone()\n",
        "            images_[..., grid_h : grid_h + patch_H, grid_w : grid_w + patch_W] = mean\n",
        "            batch_images.append(images_)\n",
        "            batch_ids.append(ids)\n",
        "        batch_images = torch.cat(batch_images, dim=0)\n",
        "        batch_ids = torch.cat(batch_ids, dim=0)\n",
        "        scores = model(batch_images).detach().gather(1, batch_ids)\n",
        "        scoremaps += list(torch.split(scores, B))\n",
        "\n",
        "    diffmaps = torch.cat(scoremaps, dim=1) - baseline\n",
        "    diffmaps = diffmaps.view(B, new_H, new_W)\n",
        "\n",
        "    return diffmaps\n",
        "\n",
        "\n",
        "def occlusion_sens_gen(image_paths, class_names, model, output_dir, cuda, topk, stride, n_batches):\n",
        "    \n",
        "    device = get_device(cuda)\n",
        "    classes = class_names\n",
        "\n",
        "    # Model from torchvision\n",
        "    model = model\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Images\n",
        "    images, _ = load_images(image_paths)\n",
        "    images = torch.stack(images).to(device)\n",
        "\n",
        "    print(\"Occlusion Sensitivity:\")\n",
        "\n",
        "    patche_sizes = [10, 15, 50]\n",
        "\n",
        "    logits = model(images)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    probs, ids = probs.sort(dim=1, descending=True)\n",
        "\n",
        "    for i in range(topk):\n",
        "        for p in patche_sizes:\n",
        "            print(\"Patch:\", p)\n",
        "            sensitivity = occlusion_sensitivity(\n",
        "                model, images, ids[:, [i]], patch=p, stride=stride, n_batches=n_batches\n",
        "            )\n",
        "\n",
        "            # Save results as image files\n",
        "            for j in range(len(images)):\n",
        "                print(\"\\t#{}: {} ({:.5f})\".format(j, classes[ids[j, i]], probs[j, i]))\n",
        "\n",
        "                save_sensitivity(\n",
        "                    filename=os.path.join(\n",
        "                        output_dir, \"new\"+str(i)+\".png\"\n",
        "                    ),\n",
        "                    maps=sensitivity[j],\n",
        "                )\n",
        "\n",
        "def LRP(model, img, lrp_type = ['zero']):\n",
        "    pass\n",
        "\n",
        "def connctivity_prop(model, img):\n",
        "    pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "70a95a82-0f54-4823-b276-71dd63400e73",
      "metadata": {
        "id": "70a95a82-0f54-4823-b276-71dd63400e73"
      },
      "source": [
        "### Model init and applying interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c88571-0a8c-4dcd-855a-1ae349040e7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20c88571-0a8c-4dcd-855a-1ae349040e7a",
        "outputId": "655cc468-91c3-4914-ec36-3efb414a79fd"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee31cec2-1ec9-404b-afed-db54300e9e0a",
      "metadata": {
        "id": "ee31cec2-1ec9-404b-afed-db54300e9e0a",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Occlussion sens function calling\n",
        "# path_ = image\n",
        "# out_dir='./'\n",
        "\n",
        "# classes = [i for i in range(1000)]\n",
        "\n",
        "# occlusion_sens_gen(path_, classes, model, out_dir, cuda=1, topk= 5, stride=1, n_batches=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2104d54b-2eb3-4618-a079-861e4565e17d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "2104d54b-2eb3-4618-a079-861e4565e17d",
        "outputId": "7b8cf2e4-6f6a-45a3-9193-0c41badb6b3c"
      },
      "outputs": [],
      "source": [
        "  # All methods without occlussion function calling\n",
        "\n",
        "res, gray_res = layer_cam_gen(model, \n",
        "                               im_show(image),\n",
        "                               target_layers= [list(model.children())[1]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f352279-d602-4f55-9c22-4ffbda12684d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "7f352279-d602-4f55-9c22-4ffbda12684d",
        "outputId": "2e8c592d-6556-4fd5-be57-7cd802c84b88"
      },
      "outputs": [],
      "source": [
        "plt.imshow(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "niboSa7liLiT",
      "metadata": {
        "id": "niboSa7liLiT"
      },
      "outputs": [],
      "source": [
        "from typing import List, Callable\n",
        "\n",
        "def tensor_x_cam(input_tensor, cam):\n",
        "  return input_tensor * cam\n",
        "\n",
        "def scores_on_targets(targets: List[Callable], input: torch.Tensor):\n",
        "    with torch.no_grad():\n",
        "      logits = model(input)\n",
        "      scores = [target(output).cpu().numpy() for target, output in zip(targets, logits)]\n",
        "      return np.float32(scores)\n",
        "\n",
        "def confidence_change_apply_cam(input_tensor: torch.Tensor, grayscale_cams: np.ndarray, targets: List[Callable], model, return_diff=True):\n",
        "  modified = []\n",
        "  for i in range(input_tensor.size(0)):\n",
        "      tensor = tensor_x_cam(input_tensor[i, ...].cpu(), torch.from_numpy(grayscale_cams[i]))\n",
        "      tensor = tensor.to(input_tensor.device)\n",
        "      modified.append(tensor.unsqueeze(0))\n",
        "  modified = torch.cat(modified)\n",
        "\n",
        "  scores_on_modified = scores_on_targets(targets = targets, input = modified)\n",
        "  scores_raw = scores_on_targets(targets = targets, input = input_tensor)\n",
        "  scores_difference = scores_on_modified - scores_raw\n",
        "\n",
        "  return scores_raw,scores_on_modified, scores_difference, modified\n",
        "\n",
        "\n",
        "\n",
        "def view(input_tensor, modified_tensor, scores_difference, preservation=False):\n",
        "    input_tens = input_tensor.detach().clone() \n",
        "    modified_tens = modified_tensor.detach().clone() \n",
        "\n",
        "    input_tens = input_tens.squeeze(0).cpu().numpy().transpose((1, 2, 0))\n",
        "    modified_tens = modified_tens.squeeze(0).cpu().numpy().transpose((1, 2, 0))\n",
        "\n",
        "    print(f\"Confidence change: {100*scores_difference} %\")\n",
        "    raw_img = Image.fromarray(deprocess_image(input_tens))\n",
        "    modified_img = Image.fromarray(deprocess_image(modified_tens))\n",
        "\n",
        "    subplots = [\n",
        "        ('Source img', [(raw_img, None, None)]),\n",
        "        ('Source img & Saliency mapping', [(modified_img, None, None)])\n",
        "    ]\n",
        "    if preservation:\n",
        "          subplots = [\n",
        "        ('Before_addition', [(raw_img, None, None)]),\n",
        "        ('After addition', [(modified_img, None, None)])\n",
        "    ]\n",
        "\n",
        "    num_subplots = len(subplots)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 3))\n",
        "\n",
        "    for i, (title, images) in enumerate(subplots):\n",
        "        ax = fig.add_subplot(1, num_subplots, i + 1)\n",
        "        ax.set_axis_off()\n",
        "        for image, cmap, alpha in images:\n",
        "            ax.imshow(image, cmap=cmap, alpha=alpha)\n",
        "        ax.set_title(title)\n",
        "\n",
        "def get_target_index(model, input_tensor):\n",
        "  model.eval()\n",
        "  out = model(input_tensor.cuda())\n",
        "  _, index = torch.max(out, 1)\n",
        "  return out, index\n",
        "\n",
        "def target_outs(model, input_tensor, softmax=True):\n",
        "  #TODO: softmax targets\n",
        "  out, index = get_target_index(model, input_tensor)\n",
        "  if len(out.shape) == 1:\n",
        "    if softmax:\n",
        "      return torch.softmax(out, dim=-1)[index]\n",
        "    else:\n",
        "      return out[index]\n",
        "  else:\n",
        "    if softmax:\n",
        "      return  torch.softmax(out, dim=-1)[:, index]\n",
        "    else:\n",
        "      return out[:, index]\n",
        "\n",
        "def deprocess_image(img):\n",
        "    img = 0.1 * (img - np.mean(img)) / (np.std(img) + 1e-5) + 0.5\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return np.uint8(img * 255)\n",
        "\n",
        "def preprocess_image(\n",
        "    img: np.ndarray, mean=[\n",
        "        0.5, 0.5, 0.5], std=[\n",
        "            0.5, 0.5, 0.5]) -> torch.Tensor:\n",
        "    preprocessing = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "    return preprocessing(img.copy()).unsqueeze(0)\n",
        "\n",
        "class ClassifierOutputSoftmaxTarget:\n",
        "    def __init__(self, category):\n",
        "        self.category = category\n",
        "\n",
        "    def __call__(self, model_output):\n",
        "        if len(model_output.shape) == 1:\n",
        "            return torch.softmax(model_output, dim=-1)[self.category]\n",
        "        return torch.softmax(model_output, dim=-1)[:, self.category]\n",
        "\n",
        "# class ClassifierOutputTarget:\n",
        "#     def __init__(self, category):\n",
        "#         self.category = category\n",
        "\n",
        "#     def __call__(self, model_output):\n",
        "#         if len(model_output.shape) == 1:\n",
        "#             return model_output[self.category]\n",
        "#         return model_output[:, self.category]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zvUjlCdIXG5x",
      "metadata": {
        "id": "zvUjlCdIXG5x"
      },
      "outputs": [],
      "source": [
        "model = models.resnet18(pretrained= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xv2N0hTCXkiT",
      "metadata": {
        "id": "xv2N0hTCXkiT"
      },
      "outputs": [],
      "source": [
        "img_name = 'dog.jpeg'\n",
        "urllib.request.urlretrieve(img_url, img_name)\n",
        "image = img_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R4QkCcs4TorD",
      "metadata": {
        "id": "R4QkCcs4TorD"
      },
      "outputs": [],
      "source": [
        "image_, raw_image = preprocess(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NyfykrNTdVUY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "NyfykrNTdVUY",
        "outputId": "551dfe10-d295-4c52-f318-7ae243477a0c"
      },
      "outputs": [],
      "source": [
        "res, gray_res = guided_backprop_gen(model, \n",
        "                               im_show(image),\n",
        "                              # [list(model.children())[2]],\n",
        "                               target_layers= [model.layer4],\n",
        "                              # target_layers = [ClassifierOutputSoftmaxTarget(219)],\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wwt1ZspaoGAk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Wwt1ZspaoGAk",
        "outputId": "06e3533e-fcce-4e23-c177-0b50f52e1a97"
      },
      "outputs": [],
      "source": [
        "res, gray_res = layer_cam_gen(model, \n",
        "                               im_show(image),\n",
        "                              # [list(model.children())[2]],\n",
        "                               target_layers= [model.layer4],\n",
        "                              # target_layers = [ClassifierOutputSoftmaxTarget(219)],\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G6WgzWWEW5AO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "G6WgzWWEW5AO",
        "outputId": "cba2a202-5633-43e2-96da-9b9e83d315f6"
      },
      "outputs": [],
      "source": [
        "plt.imshow(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZyMgNqGfoF2m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "ZyMgNqGfoF2m",
        "outputId": "9c98a17f-7ab6-47ce-eaef-3142a2f84e6e"
      },
      "outputs": [],
      "source": [
        "plt.imshow(gray_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zpTvwdcfdnjW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpTvwdcfdnjW",
        "outputId": "e4c99fa5-55ef-4f4d-d89d-a2a47c01e8b3"
      },
      "outputs": [],
      "source": [
        "np.average(gray_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aVEvWzsrWIIl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVEvWzsrWIIl",
        "outputId": "a78531a1-5047-44ad-9b0e-f0cb0dfe04ee"
      },
      "outputs": [],
      "source": [
        "img = np.array(Image.open(requests.get(img_url, stream=True).raw))\n",
        "print(img.shape)\n",
        "img = cv2.resize(img, (224, 224))\n",
        "img = np.float32(img) / 255\n",
        "input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wdnJ3GmzRR2d",
      "metadata": {
        "id": "wdnJ3GmzRR2d"
      },
      "outputs": [],
      "source": [
        "out, index = get_target_index(model, input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hq2v8zPgcyw6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "Hq2v8zPgcyw6",
        "outputId": "17f2995a-38e9-49d7-c4f6-96565cb8f8f7"
      },
      "outputs": [],
      "source": [
        "targets = [ClassifierOutputSoftmaxTarget(index[0])]\n",
        "gray_res *= 100\n",
        "scores_raw, scores_on_modified, scores_difference, modified = confidence_change_apply_cam(input_tensor.cuda(), gray_res, targets, model)\n",
        "view(input_tensor=input_tensor, modified_tensor=modified, scores_difference=scores_difference[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZK-k11c2QLjP",
      "metadata": {
        "id": "ZK-k11c2QLjP"
      },
      "outputs": [],
      "source": [
        "def deletion_metric(input_tensor, cam, model, percentile, outs=True):\n",
        "  '''change something above nth percentile with zeros, check confidence change'''\n",
        "  new_cam = np.where(cam > np.percentile(cam, percentile), 0, cam)\n",
        "  out, index = get_target_index(model, input_tensor)\n",
        "  targets = [ClassifierOutputSoftmaxTarget(index[0])]\n",
        "  score_raw, scores_on_modified, scores_difference, modified = confidence_change_apply_cam(input_tensor.cuda(), new_cam, targets, model)\n",
        "  if outs:\n",
        "    print(\"...On deletion:\", scores_on_modified)\n",
        "    view(input_tensor=input_tensor, modified_tensor=modified, scores_difference=scores_on_modified)\n",
        "  return new_cam, scores_on_modified, modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6s7Nb2Rf24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "ae6s7Nb2Rf24",
        "outputId": "bf840610-ad18-4f5d-9b91-48ae6244df04"
      },
      "outputs": [],
      "source": [
        "d, _, _ = deletion_metric(input_tensor.cuda(), gray_res, model, percentile=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VpHFCRGF1MM9",
      "metadata": {
        "id": "VpHFCRGF1MM9"
      },
      "outputs": [],
      "source": [
        "def deletion_game(input_tensor, cam, model):\n",
        "    fin_array = []\n",
        "    for perc in range(10000, 0, -10):\n",
        "      perc = perc/100\n",
        "      cam_, diff_, modified = deletion_metric(input_tensor.cuda(), gray_res, model, percentile=perc, outs=False)\n",
        "      fin_array.append([diff_, perc, cam_, modified])\n",
        "      fin_array.sort(key=lambda row: (row[0], row[1]), reverse=True)\n",
        "    diff_, perc, cam_, modified = fin_array[0]\n",
        "    print(\"...On deletion game:\")\n",
        "    view(input_tensor=input_tensor, modified_tensor=modified, scores_difference=diff_)\n",
        "    return diff_, perc, cam_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hu6qcA_Z468j",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "hu6qcA_Z468j",
        "outputId": "6b4546c7-f995-4bea-c1c1-f6464cf605e8"
      },
      "outputs": [],
      "source": [
        "diff_, perc, cam_ = deletion_game(input_tensor, cam=gray_res, model=model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XH0q1odlUZql",
      "metadata": {
        "id": "XH0q1odlUZql"
      },
      "outputs": [],
      "source": [
        "def preservation_metric(input_tensor, cam, model, left_percentile, right_percentile,outs=True):\n",
        "  '''change something above nth percentile with zeros, check confidence change'''\n",
        "  new_cam_left = np.where(cam < np.percentile(cam, left_percentile), cam, 0)\n",
        "  new_cam_right = np.where(cam < np.percentile(cam, right_percentile), cam, 0)\n",
        "  out, index = get_target_index(model, input_tensor)\n",
        "  targets = [ClassifierOutputSoftmaxTarget(index[0])]\n",
        "  score_raw_left, scores_on_modified_left, scores_difference_left, modified_left = confidence_change_apply_cam(input_tensor.cuda(), new_cam_left, targets, model)\n",
        "  score_raw_right, scores_on_modified_right, scores_difference_right, modified_right = confidence_change_apply_cam(input_tensor.cuda(), new_cam_right, targets, model)\n",
        "  scores_difference = scores_on_modified_right[0] - scores_on_modified_left[0]\n",
        "\n",
        "  if outs:\n",
        "    print(score_raw_left, scores_on_modified_left, scores_difference_left)\n",
        "    print(score_raw_right, scores_on_modified_right, scores_difference_right)\n",
        "    print(\"...On addition:\")\n",
        "    view(input_tensor=modified_left, modified_tensor=modified_right, scores_difference=scores_difference, preservation=True)\n",
        "  return scores_difference, modified_left, modified_right, new_cam_left, new_cam_right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j-mkygoxXnxE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "j-mkygoxXnxE",
        "outputId": "31cb2e58-924a-4e13-f369-eeeab0cbae80"
      },
      "outputs": [],
      "source": [
        "_, a,b,_,_ = preservation_metric(input_tensor.cuda(), gray_res, model, left_percentile=12, right_percentile=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-TOKhfGRBQt3",
      "metadata": {
        "id": "-TOKhfGRBQt3"
      },
      "outputs": [],
      "source": [
        "def preservation_game(input_tensor, cam, model):\n",
        "    fin_array = []\n",
        "    eps = 0.01\n",
        "    for perc in range(10, 10000, 10):\n",
        "      perc_l, perc_r = (perc - 10)*eps, perc*eps\n",
        "      scores_difference, new_cam_left, new_cam_right,_,_ = preservation_metric(input_tensor, cam, model, perc_l, perc_r,outs=False)\n",
        "      fin_array.append([perc_r, scores_difference, new_cam_left, new_cam_right])\n",
        "      fin_array.sort(key=lambda row: (row[1]), reverse=True)\n",
        "    perc_r, diff_, modified_left, modified_right = fin_array[0]\n",
        "    print(\"...On preservation game:\")\n",
        "    view(input_tensor=input_tensor, modified_tensor=modified_right, scores_difference=diff_, preservation=True)\n",
        "    return diff_, perc, cam_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Tfeaj0CCuSJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "6Tfeaj0CCuSJ",
        "outputId": "781a664c-d8f1-4098-88f0-9d746f6e5400"
      },
      "outputs": [],
      "source": [
        "pres_game = preservation_game(input_tensor, cam=gray_res, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z6oUVpVdaXp2",
      "metadata": {
        "id": "Z6oUVpVdaXp2"
      },
      "outputs": [],
      "source": [
        "list_tensors_ = [input_tensor, ]\n",
        "list_cams_ = [gray_res, ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ArCMTrdJaIHj",
      "metadata": {
        "id": "ArCMTrdJaIHj"
      },
      "outputs": [],
      "source": [
        "def average_drop_item(input_tensor, cam, model, index=219):\n",
        "  targets = [ClassifierOutputSoftmaxTarget(index)]\n",
        "  score_raw, scores_on_modified, scores_difference, modified = confidence_change_apply_cam(input_tensor.cuda(), cam, targets, model)\n",
        "  return max(0,-scores_difference[0])*100/score_raw[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nQ59thsIaIOT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ59thsIaIOT",
        "outputId": "302f9b10-ade4-4f48-f638-a73e9976f3d9"
      },
      "outputs": [],
      "source": [
        "average_drop_item(input_tensor, gray_res, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1mV0lAHaIVB",
      "metadata": {
        "id": "S1mV0lAHaIVB"
      },
      "outputs": [],
      "source": [
        "def avg_drop_list(list_tens, list_cam, model, index=219):\n",
        "  list_out = []\n",
        "  for tensor, cam in zip(list_tens, list_cam):\n",
        "    list_out.append(average_drop_item(tensor, cam, model, index))\n",
        "  return np.average(list_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2GmB8DWsccL5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GmB8DWsccL5",
        "outputId": "3f33e3aa-543f-423e-8e5f-ed09b4cbf654"
      },
      "outputs": [],
      "source": [
        "avg_drop_list(list_tens=list_tensors_, list_cam=list_cams_, model=model, index=219)\n",
        "#одинаково тк одно изображение в массиве"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ioI7qKeqdIes",
      "metadata": {
        "id": "ioI7qKeqdIes"
      },
      "outputs": [],
      "source": [
        "def increase_in_confidence_item(input_tensor, cam, model, index=219):\n",
        "  targets = [ClassifierOutputSoftmaxTarget(index)]\n",
        "  score_raw, scores_on_modified, scores_difference, modified = confidence_change_apply_cam(input_tensor.cuda(), cam, targets, model)\n",
        "  if scores_difference[0] > 0:\n",
        "    return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jpLd1jMldqOz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpLd1jMldqOz",
        "outputId": "c9ef3aa2-63e0-4329-ee9d-6e70cbae170d"
      },
      "outputs": [],
      "source": [
        "increase_in_confidence_item(input_tensor, gray_res, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xe5CWgR2dtH9",
      "metadata": {
        "id": "Xe5CWgR2dtH9"
      },
      "outputs": [],
      "source": [
        "def increase_in_confidence_list(list_tens, list_cam, model, index=219):\n",
        "  list_out = []\n",
        "  for tensor, cam in zip(list_tens, list_cam):\n",
        "    list_out.append(increase_in_confidence_item(tensor, cam, model, index))\n",
        "  return np.average(list_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZQSqsTeud0Q_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQSqsTeud0Q_",
        "outputId": "32162cf0-8779-4401-944a-c77790b0b5e5"
      },
      "outputs": [],
      "source": [
        "increase_in_confidence_list(list_tens=list_tensors_, list_cam=list_cams_, model=model, index=219)\n",
        "#одинаково тк одно изображение в массиве"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vs5qPNeaKOsz",
      "metadata": {
        "id": "Vs5qPNeaKOsz"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "kOS4aTN2LNkx",
      "metadata": {
        "id": "kOS4aTN2LNkx"
      },
      "source": [
        "###SPARSITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8EsKtjcPKbKG",
      "metadata": {
        "id": "8EsKtjcPKbKG"
      },
      "outputs": [],
      "source": [
        "gr_r = gray_res.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yigL-oXrLTKg",
      "metadata": {
        "id": "yigL-oXrLTKg"
      },
      "outputs": [],
      "source": [
        "gr_r_norm = gr_r - np.min(gr_r)/ np.max(gr_r) - np.min(gr_r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GtmInYFhLYng",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtmInYFhLYng",
        "outputId": "51b1509a-97c0-453b-a101-97594a4eafc5"
      },
      "outputs": [],
      "source": [
        "def sparsity(cam):\n",
        "  gr_r = cam.copy()\n",
        "  gr_r_norm = gr_r - np.min(gr_r)/ np.max(gr_r) - np.min(gr_r)\n",
        "  return 1/np.mean(gr_r_norm)\n",
        "\n",
        "print(f\" Sparsity = {sparsity(cam=gray_res)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
